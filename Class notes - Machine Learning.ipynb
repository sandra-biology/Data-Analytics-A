{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55b4369",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "- growing technology which enables computers to learn from past experiences\n",
    "- AI vs machine learning\n",
    "    - machine learning is subset of AI\n",
    "        - development of algorithms and statistical models that enable learning from past experiences\n",
    "    - definition coined in the 1950s: machine learning enables machine to automatically learn from data, improve performance from experiences and predict things without being explicitly programmed\n",
    "- traditional programming vs machine learning\n",
    "    - traditional programming: data + program -> computer -> output\n",
    "    - machine learning: data + output -> computer -> program\n",
    "- classifications of machine learning\n",
    "    - supervised learning\n",
    "    - unsupervised learning\n",
    "    - reinforcement learning\n",
    "- regression vs classification\n",
    "    - regression: uses machine learning algorithms to learn continuous mapping function\n",
    "    - classification: algorithms want to learn discrete mapping function\n",
    "- types of unsupervised learning\n",
    "    - dimensionality reduction: reduces number of dimensions from high to low while retaining critical data\n",
    "    - no labeled data\n",
    "- reinforcement learning: feedback learning method\n",
    "    - right action gets a reward\n",
    "    - wrong action gets a penalty\n",
    "- batch vs onlien learning: based on production\n",
    "    - batch (offline, then deployed): system cannot learn incrementally\n",
    "        - batches of data\n",
    "    - online learning: training happens incrementally as data arrives\n",
    "        - good option if you have limited computer resources\n",
    "- instance-based vs model-based learning\n",
    "    - instance-based: involves memorizing training data\n",
    "    - model-based: constructs models from training data\n",
    "- machine learning process: data collection -> data exploration -> data preparation -> modeling -> evaluation (circles back to modeling, iterate to find best model)\n",
    "    - 80% of the time is usually spent in data collection, data exploration, data preparation stages\n",
    "    - steps\n",
    "        - frame the problem\n",
    "        - gather data\n",
    "        - data preprocessing/cleaning\n",
    "        - model training/evaluation\n",
    "        - model deployment\n",
    "        - model testing\n",
    "        - model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42313cad",
   "metadata": {},
   "source": [
    "#### Regression analysis\n",
    "- regression: supervised learning technique which helps in finding correlation between variables\n",
    "    - mainly used for prediction\n",
    "- multicolinearity: independent variables are high correlated with each other\n",
    "- overfitting: algorithm works well with training dataset, but nto test dataset\n",
    "- underfitting: algorithm does not even work well with training dataset\n",
    "- goal with linear regression is to find best fit line where distance between measured and predicted values is minimized\n",
    "- ordinary least squares is method used for determining best fit line in linear regression\n",
    "- gradient descent\n",
    "    - iterative algorithm to train machine learning and deep learning methods\n",
    "    - helps in finding the local minimum of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245c6fc",
   "metadata": {},
   "source": [
    "#### Supervised machine learning\n",
    "- linear regression is supervised learning\n",
    "- Scikit Learn: library containing many machine learning algorithms\n",
    "    - algorithms imported, fitted\n",
    "    - users can swap algorithms in and out and test various approaches\n",
    "- regression metrics\n",
    "    - mean absolute error: calculates absolute difference between actual and predicted\n",
    "        - same unit as output variable\n",
    "        - most robust to outliers\n",
    "    - mean squared error: finds squared difference\n",
    "        - graph is differiable\n",
    "        - unit is squared unit of output variable\n",
    "        - penalizes outliers the most\n",
    "    - root mean squared error: square root of mean squared error\n",
    "        - have to use NumPy square root function on mean squared error\n",
    "- often for linear regression it's good to calculate residuals, not just metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edcf15f",
   "metadata": {},
   "source": [
    "#### Regression and correlation\n",
    "- mupltiple linear regression: extension of simple linear regression as it takes more than one predictor variable to predict the response variable\n",
    "- polynomial regression\n",
    "    - extension of linear regression\n",
    "    - models relationship between independent variable and dependent variable as nth degree polynomial\n",
    "- correlation: need to convert categorical columns to numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3873e8f",
   "metadata": {},
   "source": [
    "#### Bias-variance trade-off\n",
    "- bias: prediction error introduced in the model due to oversimplifying the machine learning algorithms\n",
    "- variance: occurs when the model performs well with training but not test dataset\n",
    "- bias is oversimplified, variance is overcomplicated\n",
    "- classification algorithm: supervised learning, identifies category of new observations into classes or groups on the basis of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cd45f",
   "metadata": {},
   "source": [
    "#### Logistic regression\n",
    "- classification algorithm\n",
    "- predicts output of a categorical depedent variable, gives probablistic values between 0 and 1\n",
    "- curve indicates the likelihood of something\n",
    "- uses continuous and discrete datasets\n",
    "- using the sigmoid function we can convert continuous data into categorical/discrete data\n",
    "- validation dataset used to\n",
    "    - evaluate model during training\n",
    "    - tune model hyperparameters\n",
    "    - pick best version of the model\n",
    "- imputation: process of filling in missing values\n",
    "- need good amount of data for the train dataset\n",
    "- continuous variables need to be scaled if they're on different ranges\n",
    "- categorical variables need to be encoded if their values are not numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d779f3",
   "metadata": {},
   "source": [
    "#### K-nearest neighbors\n",
    "- similarity between new case and available cases\n",
    "- classifies new case/data point based on similarity\n",
    "- advantages\n",
    "    - simple to implement\n",
    "    - can be more effective if training set is large\n",
    "- disadvantages\n",
    "    - always need to determine k\n",
    "    - high computaiton cost because distance needs to be computed between data points for all training samples\n",
    "- hyperparameters: parameters whose values control the learning process and determine values of model parameters\n",
    "    - k in k-nearest neighbors\n",
    "- grid search: a way of training and validating a model on every possible combination of multiple hyperparameter options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d97d1c",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM)\n",
    "- one of the most popular supervised learning algorithms\n",
    "- goal is to create decision boundary that can segregate n-dimensional space into classes/categories\n",
    "    - best decision boundary is called hyperplane\n",
    "        - dimensions of hyperplane depend on features\n",
    "- goal with linear SVM: optimal hyperplane maximizes margin/distances between classes\n",
    "- support vector classifier allows some samples to be present on the margin's or even hyperplane's wrong side\n",
    "- kernel: mathematical function which increaess dimensions\n",
    "- grid search is used to check multiple hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11482cba",
   "metadata": {},
   "source": [
    "#### Decision trees\n",
    "- decision tree learning refers to statistical modeling that uses a form of decision trees where node spltis are decided based on an informatiion matrix\n",
    "- supervised learning technique mostly used for classification problems\n",
    "- tree structure classifier\n",
    "    - decision node is parent node\n",
    "    - leaf node is child node\n",
    "    - root node starts the decision tree\n",
    "        - every root node is parent node\n",
    "        - not every parent node is root node\n",
    "- working of the algorithm\n",
    "    - starts with root node\n",
    "    - compares root attribute to record (dataset) attribute and depending on the result of this comparison chooses which branch to follow\n",
    "- gini impurity is used to guide attribute selection\n",
    "    - choose lowest impurity value to split on first\n",
    "        - choosing the feature that best splits the data into pure classes\n",
    "- we can mandate a max dept of our decision tree\n",
    "- continuous feature - first sort data\n",
    "- feature engineering is used to transform categorical columns into numerical\n",
    "- model will create single decision tree from the train dataset, then it will traverse the decision tree using the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765d37a",
   "metadata": {},
   "source": [
    "#### Random forests\n",
    "- extension of decision tree\n",
    "- decision tree limitations\n",
    "    - no guarantee of using all features\n",
    "    - root node has huge influence\n",
    "- in random forests we create random subsets of randomly picked features at each potential split\n",
    "- random forest: ensemble fo multiple decision trees, each of which is created from a subset of our dataset\n",
    "    - when we give the model test dataset, it will traverse each tree before giving the output (our prediction values)\n",
    "- hyperparameteres new to random forest\n",
    "    - n_estimators\n",
    "    - max_features\n",
    "    - bootstrap samples\n",
    "    - oob_score/out of bag error\n",
    "- with higher than certain number of trees\n",
    "    - different random selections don't reveal new information\n",
    "    - duplicate trees\n",
    "- out of bag error\n",
    "    - rows not included in bootstrapping are used to check performance\n",
    "    - hyperparameter that doesn't have real effect on training process\n",
    "    - it is optional way of measuring performance\n",
    "    - alternative to doing train/test split because bootstrapping by its nature doesn't use all rows when tree/forest is created during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7fc93",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "- used to improve performance of weak learners\n",
    "- attempts to build strong classifier from a number of weak (simple) classifiers\n",
    "    - using weak models in series\n",
    "- advantages\n",
    "    - improved accuracy\n",
    "    - better handling of imbalanced (large difference in chance between categories) data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
